{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a database of operations archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "rarc_ops_db = '/home/sbf000/ss6/rarc_ops_db.csv'\n",
    "\n",
    "if os.path.isfile(rarc_ops_db):\n",
    "    print('reading database')\n",
    "    operation_database_df = pd.read_csv(rarc_ops_db)\n",
    "else:\n",
    "    print('creating database')\n",
    "    index = '/home/smco503/Master_Protocoles/protocole_operation'\n",
    "\n",
    "    with open(index,'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "\n",
    "    hall = 'PPP5'\n",
    "    operation_database_df = pd.DataFrame([{'directory':f\"{l.split()[7].replace('${archiveHall}',hall).strip()}/{l.split()[0].replace('&','').strip()}/\",'id':l.split()[0].replace('&','').strip(), 'trunk':l.split()[4].strip(), 'branch':l.split()[5].strip()} for l in  lines if (l.split()[0][0] != '#') and (len(l.split())>=7)])\n",
    "    # operation_database_df.sort_values('trunk')\n",
    "    operation_database_df.to_csv('/home/sbf000/ss6/rarc_ops_db.csv', index=False)\n",
    "\n",
    "operation_database_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "operation_database_df.loc[operation_database_df.branch.str.contains('glbgrib')].sort_values(['trunk','branch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the available files for a branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def get_files_for_branch(operation_database_df, trunk:str, branch:str) -> list:\n",
    "    directory = operation_database_df.loc[operation_database_df.branch == branch].iloc[0].directory\n",
    "    sources = np.unique([directory + f for f in os.listdir(directory) if re.search(r'[0-9]{6}$', f) and (f[0] != '.')])\n",
    "    display([f for f in os.listdir(directory)])\n",
    "    data = []\n",
    "    for f in sources:\n",
    "        with open(f,'r') as fp:\n",
    "            data = data + fp.readlines()\n",
    "    files = ','.join([l.split()[0].strip() for l in data])\n",
    "    return files\n",
    "\n",
    "    # # get the file names from the all the read files\n",
    "    # files_df.loc[files_df.directory == directory,'files'] = ','.join([l.split()[0].strip() for l in data])\n",
    "\n",
    "\n",
    "\n",
    "def get_min_max_dates(files):\n",
    "    sorted_files = np.sort([f[0:10] for f in files.split(',')])\n",
    "    # display(sorted_files)\n",
    "    if (sorted_files.size > 0):\n",
    "        if sorted_files[0] != '':\n",
    "            min = pd.to_datetime(f'{sorted_files[0][0:8]}T{sorted_files[0][8:10]}')\n",
    "            max = pd.to_datetime(f'{sorted_files[-1][0:8]}T{sorted_files[-1][8:10]}')\n",
    "            return min,max\n",
    "    return np.nan,np.nan\n",
    "\n",
    "\n",
    "\n",
    "# files = get_files_for_branch(operation_database_df,'forecasts','lam.nat.pres')\n",
    "# print(files)\n",
    "# get_min_max_dates(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = operation_database_df.loc[operation_database_df.trunk.str.contains('grib2')]\n",
    "display(sub_df)\n",
    "for i in sub_df.index:\n",
    "    files = get_files_for_branch(sub_df,sub_df.at[i,'trunk'],sub_df.at[i,'branch'])\n",
    "# files = get_files_for_branch(sub_df,'ncep','grib.sref')\n",
    "# np.sort(files.split(','))\n",
    "# [f for f in files.split(',') if '20200111' in f]\n",
    "\n",
    "    \n",
    "    min_date,max_date = (get_min_max_dates(files))\n",
    "    sub_df.at[i,'min_date'] = min_date\n",
    "    sub_df.at[i,'max_date'] = max_date\n",
    "\n",
    "# sub_df.loc[~sub_df.min_date.isna()][['trunk','branch','min_date','max_date']].sort_values(by='trunk').reset_index(drop=True)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.loc[~sub_df.min_date.isna()].sort_values('min_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = operation_database_df.loc[operation_database_df.trunk.str.contains('observations') & (operation_database_df.branch.str.contains('grib'))]\n",
    "sub_df\n",
    "# files = get_files_for_branch(sub_df,'operations','reghyb')\n",
    "# np.sort(files.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = 48\n",
    "for i in [0,6,12,18]:\n",
    "    sum = i + 24\n",
    "    while (sum <= cap) and (sum//24==1):\n",
    "        print(sum)\n",
    "    # for j in range(0,49):\n",
    "    #     print(j)\n",
    "        # if (i + j) < 48:\n",
    "        #     # print((i+j)//24)\n",
    "        #     if (i+j) == 24:\n",
    "        #         print(f'{i:02d}_{j:03d}')\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = operation_database_df.loc[operation_database_df.branch.str.contains('hrdps')]\n",
    "for i in sub_df.index:\n",
    "    files = get_files_for_branch(sub_df,sub_df.at[i,'trunk'],sub_df.at[i,'branch'])\n",
    "    # print(files)\n",
    "    min_date,max_date = (get_min_max_dates(files))\n",
    "    sub_df.at[i,'min_date'] = min_date\n",
    "    sub_df.at[i,'max_date'] = max_date\n",
    "\n",
    "sub_df.loc[~sub_df.min_date.isna()][['trunk','branch','min_date','max_date']].sort_values(by='trunk').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rdsps.netcdf archive contents for the last 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter files on those hours\n",
    "# filtered_files = [f for f in files if f[11:14] in ['000','024','030','036','042','048']]\n",
    "filtered_files = files\n",
    "# display('filtered files')\n",
    "# display(filtered_files)\n",
    "# display(f'{len(filtered_files)} available filtered files')\n",
    "\n",
    "# find the unique dates\n",
    "unique_dates = np.sort(np.unique([f'{f[0:8]}' for f in filtered_files.split(',')]))\n",
    "\n",
    "# create a date_string for directives\n",
    "unique_dates_str = [f'{d[0:4]},{d[4:6]},{d[6:8]},{d[0:4]},{d[4:6]},{d[6:8]}' for d in unique_dates]\n",
    "# display('unique dates')\n",
    "# display(unique_dates)\n",
    "\n",
    "\n",
    "# create a dataframe of directives\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'target':np.full_like(unique_dates,'/home/sbf000/ss6',dtype='<U16'), \n",
    "        'filter':np.full_like(unique_dates,'/home/sbf000/src/rarc/filter.sh', dtype='<U31') , \n",
    "        'postprocess':np.full_like(unique_dates,'nopost',dtype='<U6'), \n",
    "        'date':unique_dates_str,\n",
    "        'branche':np.full_like(unique_dates,'operation.forecasts.lam.nat.pres',dtype='<U32'),\n",
    "        'ext':np.full_like(unique_dates,np.nan), \n",
    "        'heure':np.full_like(unique_dates,np.nan), \n",
    "        'priority':np.full_like(unique_dates,'online',dtype='<U6'),\n",
    "        'inc':np.full_like(unique_dates,'1',dtype='<U1'),\n",
    "        'inc':np.full_like(unique_dates,'#',dtype='<U1'),\n",
    "        'real_date':unique_dates,\n",
    "    })\n",
    "\n",
    "# convert date string to real datetime object for searching\n",
    "df['search_date'] = pd.to_datetime(df['real_date'])\n",
    "\n",
    "# fill the ext and heure columns of the dataframe\n",
    "for d in unique_dates:\n",
    "    df.loc[df.real_date==d,'ext'] = ','.join(np.sort(np.unique([f'{f[11:]}' for f in filtered_files if f[0:8] == d])).tolist())\n",
    "    df.loc[df.real_date==d,'heure'] = ','.join(np.sort(np.unique([f'{f[8:10]}' for f in filtered_files if f[0:8] == d])).tolist())\n",
    "\n",
    "# remove_real_date column    \n",
    "df.drop(columns='real_date',inplace=True)\n",
    "\n",
    "# create a 5 year date mask\n",
    "mask = (df['search_date'] > '2017-1-1') & (df['search_date'] <= '2021-1-1')\n",
    "\n",
    "# create a dataframe for that 5 year period\n",
    "reduced_df = df.loc[mask].drop(columns='search_date')\n",
    "\n",
    "len(reduced_df)\n",
    "\n",
    "reduced_df\n",
    "mydict = reduced_df.iloc[0].to_dict()\n",
    "\n",
    "for k,v in mydict.items():\n",
    "    print(f'{k} = {v}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d61f2654caaec969f4c4df74adf2e9158ac4acf43f2427d6ecf28f37b816665"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
